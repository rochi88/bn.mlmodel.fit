(window.webpackJsonp=window.webpackJsonp||[]).push([[6],{488:function(t,s,a){t.exports=a.p+"assets/img/blog.61c8846a.png"},514:function(t,s,a){"use strict";a.r(s);var n=a(10),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"বাংলা-ব্লগ-পোস্ট-ক্লাসিফিকেশন"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#বাংলা-ব্লগ-পোস্ট-ক্লাসিফিকেশন"}},[t._v("#")]),t._v(" বাংলা ব্লগ পোস্ট ক্লাসিফিকেশন")]),t._v(" "),n("p",[n("img",{attrs:{src:a(488),alt:"feat-image"}})]),t._v(" "),n("p",[t._v("সেন্টিমেন্ট অ্যানালাইসিস অনেক গতানুগতিক সমস্যা বিধায় আমি বাংলা ব্লগ পোস্ট ক্লাসিফিকেশন কীভাবে করতে হয় সেটা দেখাব। Natural Language Processing টেকনিক গুলো কীভাবে যেকোন ভাষায় ব্যবহার করা যায় এবং বেসিক NLP টেকনিক আমরা স্ক্র্যাচ থেকে ডেভেলপ করব।")]),t._v(" "),n("h2",{attrs:{id:"ডেটাসেট"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ডেটাসেট"}},[t._v("#")]),t._v(" ডেটাসেট")]),t._v(" "),n("p",[t._v("ডেটাসেট হিসেবে আমি চতুর্মাত্রিক ব্লগের পোস্টগুলো নিব, "),n("a",{attrs:{href:"https://github.com/manashmndl/ml.manash.me/releases/download/2.0.0/choturmatrik.json",target:"_blank",rel:"noopener noreferrer"}},[t._v("ডাউনলোড করা যাবে এখান থেকে"),n("OutboundLink")],1),t._v("।")]),t._v(" "),n("h2",{attrs:{id:"কী-কী-করতে-যাচ্ছি"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#কী-কী-করতে-যাচ্ছি"}},[t._v("#")]),t._v(" কী কী করতে যাচ্ছি")]),t._v(" "),n("ul",{staticClass:"contains-task-list"},[n("li",{staticClass:"task-list-item"},[n("p",[n("input",{staticClass:"task-list-item-checkbox",attrs:{checked:"",disabled:"",type:"checkbox"}}),t._v(" ডেটা এক্সপ্লোরেশন ও ক্লিনিং")])]),t._v(" "),n("li",{staticClass:"task-list-item"},[n("p",[n("input",{staticClass:"task-list-item-checkbox",attrs:{checked:"",disabled:"",type:"checkbox"}}),t._v(" Pandas দিয়ে বেসিক ক্লিনিং ও এক্সপ্লোরেশন")])]),t._v(" "),n("li",{staticClass:"task-list-item"},[n("p",[n("input",{staticClass:"task-list-item-checkbox",attrs:{checked:"",disabled:"",type:"checkbox"}}),t._v(" ফিল্টারিং ও টোকেনাইজেশন")])]),t._v(" "),n("li",{staticClass:"task-list-item"},[n("p",[n("input",{staticClass:"task-list-item-checkbox",attrs:{checked:"",disabled:"",type:"checkbox"}}),t._v(" ভোকাবুলারি তৈরি করা")])]),t._v(" "),n("li",{staticClass:"task-list-item"},[n("p",[n("input",{staticClass:"task-list-item-checkbox",attrs:{checked:"",disabled:"",type:"checkbox"}}),t._v(" টেক্সট থেকে ফিচার এক্সট্র্যাকশন")])]),t._v(" "),n("li",{staticClass:"task-list-item"},[n("p",[n("input",{staticClass:"task-list-item-checkbox",attrs:{checked:"",disabled:"",type:"checkbox"}}),t._v(" ব্যাগ অফ ওয়ার্ডস (BoW)")])]),t._v(" "),n("li",{staticClass:"task-list-item"},[n("p",[n("input",{staticClass:"task-list-item-checkbox",attrs:{disabled:"",type:"checkbox"}}),t._v(" টার্ম ফ্রিকোয়েন্সি ইনভার্স ডকুমেন্ট ফ্রিকোয়েন্সি (TF-IDF)")])]),t._v(" "),n("li",{staticClass:"task-list-item"},[n("p",[n("input",{staticClass:"task-list-item-checkbox",attrs:{checked:"",disabled:"",type:"checkbox"}}),t._v(" বেসিক সফটম্যাক্স ক্লাসিফায়ার দিয়ে ক্লাসিফিকেশন")])]),t._v(" "),n("li",{staticClass:"task-list-item"},[n("p",[n("input",{staticClass:"task-list-item-checkbox",attrs:{disabled:"",type:"checkbox"}}),t._v(" Word Vectors")])]),t._v(" "),n("li",{staticClass:"task-list-item"},[n("p",[n("input",{staticClass:"task-list-item-checkbox",attrs:{disabled:"",type:"checkbox"}}),t._v(" ওয়ার্ড কো অকারেন্স ম্যাট্রিক্সের উপর সিঙ্গুলার ভ্যালু ডিকোম্পোজিশন (SVD) অ্যাপ্লাই করে ফিচার এক্স্ট্র্যাকশন")])]),t._v(" "),n("li",{staticClass:"task-list-item"},[n("p",[n("input",{staticClass:"task-list-item-checkbox",attrs:{disabled:"",type:"checkbox"}}),t._v(" কন্টিনিউয়াস ব্যাগ অফ ওয়ার্ডস (CBOW) ব্যবহার করে টেন্সরফ্ল‌োতে ওয়ার্ড ভেক্টর তৈরি করা")])]),t._v(" "),n("li",{staticClass:"task-list-item"},[n("p",[n("input",{staticClass:"task-list-item-checkbox",attrs:{disabled:"",type:"checkbox"}}),t._v(" ওয়ার্ড ভেক্টর তৈরি করে ডিপ নিউরাল নেটওয়ার্ক দিয়ে ক্লাসিফিকেশন")])]),t._v(" "),n("li",{staticClass:"task-list-item"},[n("p",[n("input",{staticClass:"task-list-item-checkbox",attrs:{disabled:"",type:"checkbox"}}),t._v(" RNN দিয়ে ক্লাসিফিকেশন")])])]),t._v(" "),n("h2",{attrs:{id:"ডেটা-এক্সপ্লোরেশন"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ডেটা-এক্সপ্লোরেশন"}},[t._v("#")]),t._v(" ডেটা এক্সপ্লোরেশন")]),t._v(" "),n("p",[t._v("পানডাস দিয়ে প্রথমে JSON ফাইলটা রিড করব, তারপর ফাঁকা স্ট্রিং কোন কোন সারিতে আছে সেটা বের করতে হবে।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\ndf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_json"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./choturmatrik.json'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" encoding"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'utf-8'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("content"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("আউটপুট:")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("      content        tags\n10204               কবিতা\n12720               কবিতা\n12887               কবিতা\n13687               কবিতা\n15602          আবোল তাবোল\n")])])]),n("p",[t._v("এখন যদি এটাকে ফিল্টার আউট করতে চাই তাহলে শুধু "),n("code",[t._v("!=0")]),t._v(" বসিয়ে দিলেই হবে।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("df "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("content"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("আউটপুট:")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("                                                 content        tags\n1      অফিসে তেমন ব্যস্ততা নেই, \\nকেমন একটা নিঃসঙ্গ ভ...       কবিতা\n10     আমাদের ফেসবুক ঠিকানা: \\nগ্রুপ: Facebook.com/gr...  আবোল তাবোল\n10000  ঝিরিঝিরি বৃষ্টির অধরে \\nপয়ারের চুম্বন পাখা মেল...       কবিতা\n10008  কেন পান্থ এ চঞ্চলতা! \\nআজি শ্রাবণঘনগহন মোহে \\n...       কবিতা\n10011  মন ভাল নেই মনের ভেতর \\nমন খারাপের গন্ধ, \\nক্লা...       কবিতা\n")])])]),n("h3",{attrs:{id:"ক্লাস-বা-লেবেল-কয়টি-আছে-বের-করা"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ক্লাস-বা-লেবেল-কয়টি-আছে-বের-করা"}},[t._v("#")]),t._v(" ক্লাস বা লেবেল কয়টি আছে বের করা")]),t._v(" "),n("p",[t._v("ক্লাসের সেট করলেই বের হয়ে যাবে ইউনিক কী কী ক্লাস আছে")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("labels "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("set")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tags"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("আউটপুট:")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("['কিছু একটা লিখতে ইচ্ছে হচ্ছে', 'সমসাময়িক', 'কবিতা', 'আবোল তাবোল', 'গল্প']\n")])])]),n("h4",{attrs:{id:"ক্লাসের-নিউমেরিক-রিপ্রেজেন্টেশন"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ক্লাসের-নিউমেরিক-রিপ্রেজেন্টেশন"}},[t._v("#")]),t._v(" ক্লাসের নিউমেরিক রিপ্রেজেন্টেশন")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("index2label "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" i"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" labels"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\nlabel2index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" labels"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" i "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("h4",{attrs:{id:"ক্লাস-বেজ-স্যাম্পল-ডিস্ট্রিবিউশন"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ক্লাস-বেজ-স্যাম্পল-ডিস্ট্রিবিউশন"}},[t._v("#")]),t._v(" ক্লাস বেজ স্যাম্পল ডিস্ট্রিবিউশন")]),t._v(" "),n("p",[t._v("এবার দেখা যাক কোন ক্লাসে কতটি স্যাম্পল আছে,")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" label "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" labels"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"{:20} --- {} samples"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" label"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tags "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" label "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h4",{attrs:{id:"ক্লাসের-নিউমেরিক-ইনডেক্স-দিয়ে-ক্লাস-লেবেল-রিপ্লেস-করা"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ক্লাসের-নিউমেরিক-ইনডেক্স-দিয়ে-ক্লাস-লেবেল-রিপ্লেস-করা"}},[t._v("#")]),t._v(" ক্লাসের নিউমেরিক ইনডেক্স দিয়ে ক্লাস লেবেল রিপ্লেস করা")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tags "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tags"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("replace"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("label2index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("আউটপুট:")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("content  tags\n1      অফিসে তেমন ব্যস্ততা নেই, \\nকেমন একটা নিঃসঙ্গ ভ...     2\n10     আমাদের ফেসবুক ঠিকানা: \\nগ্রুপ: Facebook.com/gr...     3\n10000  ঝিরিঝিরি বৃষ্টির অধরে \\nপয়ারের চুম্বন পাখা মেল...     2\n10008  কেন পান্থ এ চঞ্চলতা! \\nআজি শ্রাবণঘনগহন মোহে \\n...     2\n10011  মন ভাল নেই মনের ভেতর \\nমন খারাপের গন্ধ, \\nক্লা...     2\n")])])]),n("p",[t._v("বেসিক কাজ শেষ। এবার ডেটাসেট ক্লিন করার পালা।")]),t._v(" "),n("h2",{attrs:{id:"ডেটাসেট-ক্লিনিং"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ডেটাসেট-ক্লিনিং"}},[t._v("#")]),t._v(" ডেটাসেট ক্লিনিং")]),t._v(" "),n("p",[t._v("প্রথম ১০০ ব্লগ পোস্ট প্রিন্ট করে দেখা যাক।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pprint "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pprint\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" content "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("content"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    pprint"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("content"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"----"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("আউটপুট:")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("('অফিসে তেমন ব্যস্ততা নেই, \\n'\n 'কেমন একটা নিঃসঙ্গ ভাব \\n'\n 'মনটাকে পেতে আজকে আমার \\n'\n 'দিয়েছে বদলে আগের স্বভাব। \\n'\n 'ফেলে আসা দিন গুলো ভাবাচ্ছে, \\n'\n 'স্মৃতিরা আমাকে অতিষ্ঠ করে \\n'\n 'হয়তো বা কোন প্রতিশোধ নিতে- \\n'\n 'চাচ্ছে, ওরাও জুলম করছে। \\n'\n 'হয়ত ঘোরের মধ্যে পড়ে গেছি, \\n'\n 'একজন তুমি বা কাল্পনিক - \\n'\n 'প্রেমীকাকে খুব বিনয় করছি, \\n'\n 'বলছি, আমায় তুমিও বোঝ না? \\n'\n .....\n")])])]),n("p",[t._v("সঙ্গতকারণেই সব কয়টা প্রিন্ট করা সম্ভব হচ্ছে না। প্রতিটা ব্লগ পোস্টে কি কি শব্দ আছে সেগুলো ইনডেক্সিং করতে হবে সবার আগে, আর সেটা করার জন্য প্রথমে আমাদের যতিচিহ্ন বাদ দিতে হবে।")]),t._v(" "),n("h3",{attrs:{id:"পাঙ্কচুয়েশন-ও-স্টপওয়ার্ড-রিমুভ-করা-এবং-টোকেনাইজেশন"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#পাঙ্কচুয়েশন-ও-স্টপওয়ার্ড-রিমুভ-করা-এবং-টোকেনাইজেশন"}},[t._v("#")]),t._v(" পাঙ্কচুয়েশন ও স্টপওয়ার্ড রিমুভ করা এবং টোকেনাইজেশন")]),t._v(" "),n("p",[t._v('অপ্রয়োজনীয় শব্দও বাদ দেয়া যেতে পারে, যেটাকে বলে Stopword। বাংলায় যেহেতু স্টপওয়ার্ড কালেকশন খুব কম তাই আমরা ছোট ওই কালেকশন ব্যবহার করেই ডেটাসেট ক্লিন করার চেষ্টা করব। স্টপওয়ার্ডের উদাহরণ হল, "আপনার, আমি, আমার, তুমি, পারেন, পর্যন্ত...." ইত্যাদি।')]),t._v(" "),n("p",[t._v("স্টপওয়ার্ড রিমুভ করলে আপাতদৃষ্টিতে একটা টেক্সট এর অর্থের তেমন পরিবর্তন হয় না, আর এই শব্দগুলি বেশি থাকে, তাই আমাদের মেশিন লার্নিং মডেল যাতে অপ্রয়োজনীয় শব্দের উপস্থিতিতে বায়াজড না হয়ে যায় তাই আমরা এই শব্দগুলি রিমুভ করে থাকি। পাঙ্কচুয়েশনের পাশাপাশি আমাদের বিভিন্ন ধরণের ক্যারেক্টার যেমন "),n("code",[t._v("\\n, \\t")]),t._v(" ইত্যাদি সবকিছু রিমুভ করতে হবে। স্টপওয়ার্ড লিস্ট হিসেবে আমি "),n("a",{attrs:{href:"https://raw.githubusercontent.com/explosion/spaCy/master/spacy/lang/bn/stop_words.py",target:"_blank",rel:"noopener noreferrer"}},[t._v("এটা"),n("OutboundLink")],1),t._v(" ব্যবহার করব।")]),t._v(" "),n("p",[t._v("কাজগুলো করার সময় আমি একটা সেন্টেন্স নিয়ে কাজ করব, তারপর সেটাকে একটা ফাংশনের মধ্যে র‍্যাপ করে দেব।")]),t._v(" "),n("h4",{attrs:{id:"পাঙ্কচুয়েশন-রিমুভাল"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#পাঙ্কচুয়েশন-রিমুভাল"}},[t._v("#")]),t._v(" পাঙ্কচুয়েশন রিমুভাল")]),t._v(" "),n("p",[t._v("পাঙ্কচুয়েশন রিমুভ করার জন্য পাইথনের বিল্টইন "),n("code",[t._v("str.maketrans")]),t._v(" এর সাহায্যে খুব ফাস্ট যেসব ক্যারেক্টার আমাদের দরকার নাই ওগুলিকে ‍স্পেস দ্বারা রিপ্লেস করে দেব।")]),t._v(" "),n("p",[t._v("ডেটাসেট থেকে একটা স্যাম্পল নিয়ে দেখা যাক।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Taking the first sample")]),t._v("\nsample "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("content"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sample"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("আউটপুট:")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("অফিসে তেমন ব্যস্ততা নেই,\nকেমন একটা নিঃসঙ্গ ভাব\nমনটাকে পেতে আজকে আমার\nদিয়েছে বদলে আগের স্বভাব।\nফেলে আসা দিন গুলো ভাবাচ্ছে,\nস্মৃতিরা আমাকে অতিষ্ঠ করে\nহয়তো বা কোন প্রতিশোধ নিতে-...\n")])])]),n("p",[t._v("এবার আমরা চেষ্টা করব স্যাম্পল থেকে অবাঞ্ছিত ক্যারেক্টার গুলো দূর করতে।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("filters "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n!"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n?,।!‍.0123456789০১২৩৪৫৬৭৮৯\n"""')]),t._v("\ntranslate_dict "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("c"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" c "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" filters"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntranslate_map "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("maketrans"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("translate_dict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sample"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("translate"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("translate_map"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("আউটপুট:")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("অফিসে তেমন ব্যস্ততা নেই   কেমন একটা নিঃসঙ্গ ভাব  মনটাকে পেতে আজকে আমার  দিয়েছে বদলে আগের স্বভাব   ফেলে আসা দিন গুলো ভাবাচ্ছে   স্মৃতিরা আমাকে অতিষ্ঠ করে  হয়তো বা কোন প্রতিশোধ নিতে   চাচ্ছে  ওরাও জুলম করছে   হয়ত ঘোরের মধ্যে পড়ে গেছি   একজন তুমি বা কাল্পনিক    প্রেমীকাকে খুব বিনয় করছি   বলছি  আমায় তুমিও বোঝ না   কেন কাছে আসো না ভালবাস না   তাহলে তো আর কিছুই হবে না   আমার কখনো    বিয়েই হবে না  সুখ মিলবে না    হা হা হা  দারুণ মজার তাই না   এমন মজার সময় কাটছে  তবুও আমার ভাল লাগছে না   নিজেকে বন্দী বন্দী লাগছে  সবকিছু যেন অচেনা অজানা   চেনা জানা সুখ গুলো নেই আর   কোথায় যে সব গিয়েছে হারিয়ে  কেউ তার খোঁজ হয়ত জানে না  ঠিকানা কোথায় বলতে পারে না   যৌবনে একাকী থাকা কষ্টের   একারণেই কি কাজে গতি আসে   সবাই চেষ্টা করে তারাতারি  সঙ্গিনী আর সফলতা পেতে   আমার কি তবে প্রয়োজন সেই    সফলতা আর সেই সঙ্গিনী   হয়তো বা তাই   একারণেই তো বন্য হয়ে যাই   আমার বন্যতা হরেক রকম  কান্না হাসির   পড়ার লেখার   বলার   চলার  দেবার নেবার  ঘুমের  জাগার ইত্যাদি ইত্যাদি   এর সবি আমি প্রকাশ করেছি  এর সবি হল আপন স্বভাব\n")])])]),n("p",[t._v("এখন যদি আমি একে স্পেস ডেলিমিটার দিয়ে স্প্লিট করি তাহলে আর্টিকেলের সবকয়টি শব্দের কালেকশন পেয়ে যাব।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("words "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sample"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("translate"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("translate_map"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("words"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("আউটপুট:")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("['অফিসে', 'তেমন', 'ব্যস্ততা', 'নেই', 'কেমন', 'একটা', 'নিঃসঙ্গ', 'ভাব', 'মনটাকে', 'পেতে', 'আজকে', 'আমার', 'দিয়েছে', 'বদলে', 'আগের', 'স্বভাব', 'ফেলে', 'আসা', 'দিন', 'গুলো', 'ভাবাচ্ছে', 'স্মৃতিরা', 'আমাকে', 'অতিষ্ঠ', 'করে', 'হয়তো', 'বা', 'কোন', 'প্রতিশোধ', 'নিতে', 'চাচ্ছে', 'ওরাও', 'জুলম', 'করছে', 'হয়ত', 'ঘোরের', 'মধ্যে', 'পড়ে', 'গেছি', 'একজন', 'তুমি', 'বা', 'কাল্পনিক', 'প্রেমীকাকে', 'খুব', 'বিনয়', 'করছি', 'বলছি', 'আমায়', 'তুমিও', 'বোঝ', 'না', 'কেন', 'কাছে', 'আসো', 'না', 'ভালবাস', 'না', 'তাহলে', 'তো', 'আর', 'কিছুই', 'হবে', 'না', 'আমার', 'কখনো', 'বিয়েই', 'হবে', 'না', 'সুখ', 'মিলবে', 'না', 'হা', 'হা', 'হা', 'দারুণ', 'মজার', 'তাই', 'না', 'এমন', 'মজার', 'সময়', 'কাটছে', 'তবুও', 'আমার', 'ভাল', 'লাগছে', 'না', 'নিজেকে', 'বন্দী', 'বন্দী', 'লাগছে', 'সবকিছু', 'যেন', 'অচেনা', 'অজানা', 'চেনা', 'জানা', 'সুখ', 'গুলো', 'নেই', 'আর', 'কোথায়', 'যে', 'সব', 'গিয়েছে', 'হারিয়ে', 'কেউ', 'তার', 'খোঁজ', 'হয়ত', 'জানে', 'না', 'ঠিকানা', 'কোথায়', 'বলতে', 'পারে', 'না', 'যৌবনে', 'একাকী', 'থাকা', 'কষ্টের', 'একারণেই', 'কি', 'কাজে', 'গতি', 'আসে', 'সবাই', 'চেষ্টা', 'করে', 'তারাতারি', 'সঙ্গিনী', 'আর', 'সফলতা', 'পেতে', 'আমার', 'কি', 'তবে', 'প্রয়োজন', 'সেই', 'সফলতা', 'আর', 'সেই', 'সঙ্গিনী', 'হয়তো', 'বা', 'তাই', 'একারণেই', 'তো', 'বন্য', 'হয়ে', 'যাই', 'আমার', 'বন্যতা', 'হরেক', 'রকম', 'কান্না', 'হাসির', 'পড়ার', 'লেখার', 'বলার', 'চলার', 'দেবার', 'নেবার', 'ঘুমের', 'জাগার', 'ইত্যাদি', 'ইত্যাদি', 'এর', 'সবি', 'আমি', 'প্রকাশ', 'করেছি', 'এর', 'সবি', 'হল', 'আপন', 'স্বভাব']\n")])])]),n("p",[t._v("সুতরাং আমরা একটা ফাংশন লিখতে পারি!।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("sentence_to_wordlist")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sentence"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" filters"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"!\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n?,।!‍.0123456789০১২৩৪৫৬৭৮৯"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# for english")]),t._v("\n    sentence "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sentence"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lower"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    translate_dict "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("c"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" c "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" filters"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    translate_map "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("maketrans"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("translate_dict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" sentence"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("translate"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("translate_map"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sentence_to_wordlist"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("content"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("আউটপুট:")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("['ঝিরিঝিরি',\n 'বৃষ্টির',\n 'অধরে',\n 'পয়ারের',\n 'চুম্বন',\n 'পাখা',\n 'মেলেছে...\n")])])]),n("p",[t._v("এবার স্টপওয়ার্ড রিমুভ করতে হবে, তাহলে শুধু চেক করতে হবে যে কোন একটি শব্দ স্টপওয়ার্ড লিস্টে আছে কিনা, যদি থাকে তাহলে ওইটা রিটার্ন করবে না, যদি না থাকে তাহলে শব্দটি স্টপওয়ার্ড না এবং মূল ওয়ার্ডলিস্টে স্থান পাবে।")]),t._v(" "),n("p",[t._v("এটা করার জন্য আমাদের পূর্বের ফাংশনে "),n("code",[t._v("filter")]),t._v(" ফাংশনটি কল করে একটি Lambda ফাংশন পাস করব।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("filter")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" STOP_WORDS"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordlist"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("সুতরাং পরিবর্তিত ফাংশন,")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("sentence_to_wordlist")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sentence"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" filters"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"!\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n?,।!‍.\'0123456789০১২৩৪৫৬৭৮৯‘\\u200c–“”…‘"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    translate_dict "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("c"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" c "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" filters"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    translate_map "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("maketrans"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("translate_dict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    wordlist "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sentence"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("translate"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("translate_map"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("filter")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" STOP_WORDS"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordlist"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("তাহলে এই ফাংশনটি প্রথমে স্টপওয়ার্ড রিমুভ করে, তারপর টোকেনাইজ করে এবং সবশেষে স্টপওয়ার্ড রিমুভ করে।")]),t._v(" "),n("h4",{attrs:{id:"ডকুমেন্ট"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ডকুমেন্ট"}},[t._v("#")]),t._v(" ডকুমেন্ট")]),t._v(" "),n("p",[t._v("একেকটা ডকুমেন্ট বলতে আমরা কোন কন্টেন্টের ওয়ার্ডলিস্টকে বুঝাব। যেমন,")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("['অফিসে', 'তেমন', 'ব্যস্ততা', 'নেই', 'কেমন', 'একটা', 'নিঃসঙ্গ', 'ভাব', 'মনটাকে', 'পেতে', 'আজকে', 'আমার', 'দিয়েছে', 'বদলে', 'আগের', 'স্বভাব', ..... ]\n")])])]),n("p",[t._v("এটা একটা ডকুমেন্ট। এইভাবে ডকুমেন্টের বড় লিস্ট হল কালেকশন অফ ডকুমেন্টস।")]),t._v(" "),n("h2",{attrs:{id:"ভোকাবুলারি-তৈরি-করা"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ভোকাবুলারি-তৈরি-করা"}},[t._v("#")]),t._v(" ভোকাবুলারি তৈরি করা")]),t._v(" "),n("p",[t._v("আমাদের কাজের বিবেচনায় ভোকাবুলারি হল এমন একটি কন্টেইনার যেখানে প্রতিটা ওয়ার্ডের জন্য একটি ইনডেক্স অ্যাসাইন করা থাকবে, দিনশেষে আমরা ইন্টিজার নিয়েই কাজ করব, সেখানে ওয়ার্ড বলতে কিছু থাকবে না। লেবেল ইনডেক্সিংয়ের মত করেই ওয়ার্ড ইনডেক্স করব। ভোকাবুলারি মানেই সেখানে শুধু ইউনিক ওয়ার্ড পাওয়া যাবে।")]),t._v(" "),n("p",[t._v("একটা ক্লাস লিখব, যাতে সেখানে প্রয়োজনীয় ফাংশন থাকে, যেমন BoW, TF-IDF তৈরি করার মত ইউটিলস।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Vocabulary")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("object")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" documents"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token2id "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" defaultdict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id2token "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" defaultdict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# token count in a document")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dfs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_collection "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("set")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_counter "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Counter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("documents "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" documents "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("update_documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("update_token_dictionary")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" remove_previous"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" remove_previous"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token2id "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" defaultdict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id2token "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" defaultdict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token2id"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("update"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id2token"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("update"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id2token"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("update"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" idx"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" word "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" idx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" word "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_collection"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token2id"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("update"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" word"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" idx"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" idx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" word "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_collection"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("update_documents")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Extending the documents")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extend"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" idx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" document "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" word "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" document"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_counter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n                self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_collection"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("word"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("update_token_dictionary"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# add_documents removes the previous ones")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("add_documents")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("documents "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_counter "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Counter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token2id "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" defaultdict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id2token "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" defaultdict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("update_documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("__len__")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id2token"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token2id"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id2token"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_token_count")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" token"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" token "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token2id"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("raise")]),t._v(" ValueError"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Token doesn\'t exist"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_counter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("token"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("reduce_vocabulary_by_frequency")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" threshold"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        token_collection "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" tok "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" tok "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_counter "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_counter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("tok"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" threshold "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_collection "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("set")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("token_collection"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("update_token_dictionary"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("remove_previous"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("__getitem__")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id2token"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("raise")]),t._v(" KeyError"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Key can\'t be equal or greater than total token count"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id2token"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),n("p",[t._v("এই ক্লাসে বেশ কিছু ফাংশন লিখে ফেললাম, প্রতিটা ফাংশনের কাজ একে একে ব্যাখ্যা করা হবে।")]),t._v(" "),n("h4",{attrs:{id:"add-documents"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#add-documents"}},[t._v("#")]),t._v(" "),n("code",[t._v("add_documents")])]),t._v(" "),n("p",[t._v("এই ফাংশনে ডকুমেন্ট পাস করলে আগের ডকুমেন্টের ডেটা (যদি অ্যাড করা থাকে) সব মুছে যাবে ও টোকেন কালেকশন, তাদের আইডি নতুনভাবে তৈরি হবে। এই ফাংশনের কাজ হল প্রতিটা টোকেন ভোকাবুলারিতে অ্যাড করা ও ওই টোকেনটি সর্বমোট কতবার দেখা গেছে তার কাউন্ট "),n("code",[t._v("token_counter")]),t._v(" এ স্টোর করা।")]),t._v(" "),n("h4",{attrs:{id:"update-documents"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#update-documents"}},[t._v("#")]),t._v(" "),n("code",[t._v("update_documents")])]),t._v(" "),n("p",[t._v("যদি আমাদের একবার ডকুমেন্ট অ্যাড করার পরে আবারও ডকুমেন্ট অ্যাড করার দরকার হয় তাহলে এই ফাংশনটা কল করলে আগের ডকুমেন্টের সাথে নতুন ডকুমেন্ট অ্যাড হবে, মানে যদি নতুন ডকুমেন্টে নতুন শব্দ থাকে সেটাও ভোকাবুলারিতে অ্যাড হয়ে যাবে ও ইনডেক্সিংও আপডেটেড হবে।")]),t._v(" "),n("h4",{attrs:{id:"reduce-vocabulary-by-frequency"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#reduce-vocabulary-by-frequency"}},[t._v("#")]),t._v(" "),n("code",[t._v("reduce_vocabulary_by_frequency")])]),t._v(" "),n("p",[t._v("অনেক সময় দেখা যায় অপ্রয়োজনীয় ওয়ার্ড অনেকবার ডকুমেন্টের কালেকশনে থাকতে পারে। Bag of Words মডেলটা অনেক Sparse হওয়ার কারণে এইসব অপ্রয়োজনীয় ওয়ার্ডের কারণে ফিচার ভেক্টর সাইজ অনেক বড় হতে পারে। এইকারণে প্রয়োজনমাফিক ভোকাবুলারি সাইজ কমানোর জন্য এই ফাংশনটা কাজে আসবে।")]),t._v(" "),n("h3",{attrs:{id:"bag-of-words-model"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#bag-of-words-model"}},[t._v("#")]),t._v(" Bag of Words Model")]),t._v(" "),n("p",[t._v("তৈরিকৃত Vocabulary ক্লাসের সাহায্যে ব্যাগ অফ ওয়ার্ডস মডেল তৈরি করব।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("docs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'আমি'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'NLP'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'পছন্দ'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'করি'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'NLP'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'মানে'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Neuro'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Linguistic'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Programming'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'না'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'NLP'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ও'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Deep'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Learning'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'এক'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'জিনিস'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'না'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nv "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Vocabulary"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("docs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nunique_tokens "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_collection"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("unique_tokens"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("unique_tokens"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("আউটপুট:")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("['Deep', 'ও', 'Learning', 'করি', 'পছন্দ', 'Linguistic', 'Programming', 'না', 'জিনিস', 'মানে', 'আমি', 'এক', 'Neuro', 'NLP'] 14\n")])])]),n("p",[t._v("আমাদের ব্যাগ অফ ওয়ার্ডস মডেলের ভেক্টর সাইজ হবে ১৪। প্রতিটা ডকুমেন্টের জন্য এটা ফিক্সড। কোন ডকুমেন্টে এই শব্দগুলার মধ্যে কোনটা কতবার আছে সেই কাউন্টটাই হবে ব্যাগ অফ ওয়ার্ডস রিপ্রেজেন্টেশন। যদি টোকেন কোন ডকুমেন্টে না থাকে তাহলে সেখানে 0 বসিয়ে দিলেই হবে।")]),t._v(" "),n("h5",{attrs:{id:"docs-0-এর-bow-রিপ্রেজেন্টেশন"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#docs-0-এর-bow-রিপ্রেজেন্টেশন"}},[t._v("#")]),t._v(" "),n("code",[t._v("docs[0]")]),t._v(" এর BoW রিপ্রেজেন্টেশন")]),t._v(" "),n("p",[t._v("প্রথম ডকুমেন্টটি হল, "),n("code",[t._v("['আমি', 'NLP', 'পছন্দ', 'করি']")]),t._v("। এখানে, যেসব শব্দগুলো অনুপস্থিত সেগুলো হচ্ছে,")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("{'Deep',\n 'Learning',\n 'Linguistic',\n 'Neuro',\n 'Programming',\n 'এক',\n 'ও',\n 'জিনিস',\n 'না',\n 'মানে'}\n")])])]),n("p",[t._v("তাহলে, আমরা যদি আমাদের টোকেন ইনডেক্সিং দেখি,")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pprint"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token2id"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("আউটপুট:")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("defaultdict(<function Vocabulary.add_documents.<locals>.<lambda> at 0x7f765c203f28>,\n            {' ': 0,\n             'Deep': 1,\n             'Learning': 3,\n             'Linguistic': 6,\n             'NLP': 14,\n             'Neuro': 13,\n             'Programming': 7,\n             'আমি': 11,\n             'এক': 12,\n             'ও': 2,\n             'করি': 4,\n             'জিনিস': 9,\n             'না': 8,\n             'পছন্দ': 5,\n             'মানে': 10})\n")])])]),n("p",[t._v("সুতরাং প্রথম ডকুমেন্টের BoW রিপ্রেজেন্টেশন,")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("[0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.]\n")])])]),n("p",[t._v("কারণ, শুরুর ইনডেক্সটা ডিফল্ট 0 হবে, এটা করার কারণ কোন একটি অপরিচিত ওয়ার্ড দিয়ে যদি আমরা রিপ্রেজেন্ট করি যেটা ভোকাবুলারিতে নেই তখন আমরা আগে 0 বসাব সেসব ওয়ার্ডের জন্য। তাই আমি "),n("code",[t._v("defaultdict")]),t._v(" তৈরি করেছি। এটার সুবিধা হল, এই ডিকশনারিতে যদি কোন "),n("code",[t._v("key")]),t._v(" না পাওয়া যায় তাহলে সে একটা ডিফল্ট ভ্যালু রিটার্ন করে। আর এখানে ডিফল্ট ভ্যালু হল 0।")]),t._v(" "),n("p",[n("code",[t._v("1")]),t._v(" ইনডেক্সে "),n("code",[t._v("Deep")]),t._v(" ওয়ার্ড আছে কিন্তু প্রথম ডকুমেন্টে "),n("code",[t._v("Deep")]),t._v(" ওয়ার্ড নাই তাই তার কাউন্ট শূন্য। এভাবে বাকি ওয়ার্ডগুলো তাদের ইন্ডেক্সের জায়গায় আমরা কাউন্ট বসিয়ে দিচ্ছি।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1 for space")]),t._v("\nbow "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("docs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("unique_tokens"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" doc "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("docs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" tok "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" doc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        bow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token2id"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("tok"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("আউটপুট:")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("array([[0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1.],\n       [0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.]])\n")])])]),n("p",[t._v("এবার এই কাজটা আমরা "),n("code",[t._v("Vocabulary")]),t._v(" ক্লাসের ফাংশন হিসেবে অ্যাড করব।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Vocabulary")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("object")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    .......\n    """')]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("bow_from_saved_documents")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        bow_matrix "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_collection"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" doc "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" tok "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" doc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                bow_matrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token2id"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("tok"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" bow_matrix\n")])])]),n("p",[t._v("এবার,")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("v "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Vocabulary"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("docs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bow_from_saved_documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("array([[0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1.],\n       [1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.]])\n")])])]),n("p",[t._v("এবার আরেকটি ফাংশন লিখতে হবে যেটা ইন্সট্যান্টলি একটা ডকুমেন্টের BoW রিপ্রেজেন্টেশন রিটার্ন করে।")]),t._v(" "),n("p",[t._v("নতুন যে রিপ্রেজেন্টেশন আসবে সেটাকে অবশ্যই পূর্বের ডকুমেন্টের উপর ফিট করা ভোকাবুলারি থেকেই আসতে হবে। যদি নতুন ডকুমেন্টে অপরিচিত শব্দ থাকে ওইটা আমরা ডিসকার্ড করব।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("bow_sample "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_collection"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nnew_doc "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'NLP'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'কঠিন'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'নাকি'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'সহজ'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" tok "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" new_doc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    bow_sample"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token2id"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("tok"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bow_sample"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("আউটপুট:")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("array([3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n")])])]),n("p",[t._v("ভোকাবুলারিতে "),n("code",[t._v("'কঠিন', 'নাকি', 'সহজ'")]),t._v(" এই তিনটা শব্দ অনুপস্থিত, তাই এগুলো অ্যারে এর প্রথমে এসে অ্যাড হয়েছে। অ্যারে সাইজ এখন 17, তাই ফিক্সড সাইজ বজায় রাখার জন্য প্রথম ইনডেক্স বাদ দিলেই হবে।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("bow_sample "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" bow_sample"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),n("p",[t._v("তাহলে আরেকটা ফাংশন লিখি যেটা দিয়ে এই কাজটা করা যাবে।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Vocabulary")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("object")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    .....\n    """')]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("doc2bow")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" doc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        bow "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_collection"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" tok "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" doc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            bow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token2id"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("tok"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" bow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("v "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Vocabulary"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("docs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nnew_doc "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'NLP'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'কঠিন'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'নাকি'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'সহজ'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("doc2bow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("new_doc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n")])])]),n("h2",{attrs:{id:"scikit-learn-দিয়ে-লিনিয়ার-সফটম্যাক্স-মডেল-তৈরি-ও-ভ্যালিডেশন"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#scikit-learn-দিয়ে-লিনিয়ার-সফটম্যাক্স-মডেল-তৈরি-ও-ভ্যালিডেশন"}},[t._v("#")]),t._v(" scikit-learn দিয়ে লিনিয়ার সফটম্যাক্স মডেল তৈরি ও ভ্যালিডেশন")]),t._v(" "),n("p",[t._v("প্রয়োজনীয় ইম্পোর্ট ও ব্যাগ অফ ওয়ার্ডস রিপ্রেজেন্টেশন")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linear_model "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" LogisticRegression\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("metrics "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" accuracy_score\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_selection "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" train_test_split\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" shuffle\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# discard tokens below this frequency threshold")]),t._v("\nFREQ_THRESHOLD "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# train test data")]),t._v("\nv "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Vocabulary"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduce_vocabulary_by_frequency"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("FREQ_THRESHOLD"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrain_x "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bow_from_saved_documents"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("train_y "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("asarray"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tags"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# shuffling")]),t._v("\ntrain_x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_y "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" shuffle"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_y"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" random_state"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("42")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# train test split")]),t._v("\nX_train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" X_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_test "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_test_split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_y"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" test_size"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.33")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" random_state"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("42")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Model creation and fitting")]),t._v("\nlr "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" LogisticRegression"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("C"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlr"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X_train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("এবার টেস্ট ডেটার উপর প্রেডিক্ট করে অ্যাকুরেসি মাপব।")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("y_pred "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lr"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("accuracy_score"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("y_true"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("y_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_pred"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("y_pred"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("আউটপুট:")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("0.7657445556209534\n")])])]),n("p",[t._v("একদম খারাপ না, তাই না?")]),t._v(" "),n("p",[t._v("এবার এই মডেল দিয়ে যদি নতুন কোন পোস্ট ক্লাসিফাই করতে হয় তাহলে?")]),t._v(" "),n("h2",{attrs:{id:"ট্রেইন্ড-মডেল-থেকে-প্রেডিকশন"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ট্রেইন্ড-মডেল-থেকে-প্রেডিকশন"}},[t._v("#")]),t._v(" ট্রেইন্ড মডেল থেকে প্রেডিকশন")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("test_string "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n  গগনে গরজে মেঘ, ঘন বরষা।\n\n     কূলে একা বসে আছি, নাহি ভরসা।\n  """')]),t._v("\n\ndoc "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sentence_to_wordlist"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_string"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nbow_of_doc "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("doc2bow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("doc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\noutput "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lr"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("bow_of_doc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("index2label"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("output"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("আউটপুট:")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("কবিতা\n")])])]),n("hr"),t._v(" "),n("p",[t._v("ব্যাগ অফ ওয়ার্ডস খুবই সাধারণ টেক্সট ভেক্টরাইজেশন প্রসেস। এরপরে আরও অ্যাডভান্সড টেকনিক দেখানো হবে যেখানে টেক্সটের সিকোয়েন্সকেও আমরা বিবেচনা করে আরও অ্যাকুরেট মডেল তৈরি করতে পারব।")]),t._v(" "),n("p",[t._v("জুপিটার নোটবুক পাওয়া যাবে "),n("a",{attrs:{href:"https://github.com/manashmndl/ml.manash.me/blob/master/nlp/BlogPostClassification.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("এখানে"),n("OutboundLink")],1),t._v("।")]),t._v(" "),n("Disqus")],1)}),[],!1,null,null,null);s.default=e.exports}}]);